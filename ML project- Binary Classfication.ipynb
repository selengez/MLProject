{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401aa74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.impute import SimpleImputer,KNNImputer\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn import model_selection, metrics\n",
    "from sklearn.model_selection import train_test_split,RandomizedSearchCV,learning_curve\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374b1adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('secondary_data.csv', delimiter= ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634d9a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4018eddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "######Data Preprocessing####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df35465",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1- defining null values\n",
    "null = df.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af86a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1f993a",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = df.isnull().sum()\n",
    "missing_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63761329",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_with_missing_values = missing_values[missing_values > 0]\n",
    "columns_with_missing_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8eabc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating rate of missing values\n",
    "percentage = df.isnull().mean()*100\n",
    "print(percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bec303d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns have null values more than 40\n",
    "df = df.drop(columns=percentage[percentage > 40].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401f8466",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop na rows from gill attachment bcs it has high null values\n",
    "df = df.dropna(subset=['gill-attachment'])\n",
    "df.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98606ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.dtypes, percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a40aace",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "numerical_cols = ['cap-diameter', 'stem-height', 'stem-width']\n",
    "categorical_cols = ['gill-attachment', 'cap-surface', 'ring-type']\n",
    "\n",
    "missing_numerical_cols = [col for col in numerical_cols if col not in df.columns]\n",
    "missing_categorical_cols = [col for col in categorical_cols if col not in df.columns]\n",
    "\n",
    "if missing_numerical_cols:\n",
    "    print(f\"Missing numerical columns: {missing_numerical_cols}\")\n",
    "else:\n",
    "    \n",
    "    imputer_num = SimpleImputer(strategy='mean')\n",
    "    df[numerical_cols] = imputer_num.fit_transform(df[numerical_cols])\n",
    "\n",
    "if missing_categorical_cols:\n",
    "    print(f\"Missing categorical columns: {missing_categorical_cols}\")\n",
    "else:\n",
    "   \n",
    "    for col in categorical_cols:\n",
    "        df[col].fillna(df[col].mode()[0], inplace=True)\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138853ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e20f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "row_count, column_count = df.shape\n",
    "\n",
    "print(f\"Number of rows: {row_count}\")\n",
    "print(f\"Number of columns: {column_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e871439f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######EDA######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e100a8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##pie chart for distribution of target value\n",
    "class_counts = df['class'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.pie(class_counts, labels=class_counts.index, autopct='%1.1f%%', colors=['tomato','skyblue'], startangle=140)\n",
    "plt.title('Distribution of Poisonous and Edible Mushrooms')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a33e667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution for numerical columns\n",
    "summary_stats = df[numerical_cols].describe()\n",
    "\n",
    "\n",
    "class_distribution = df['class'].value_counts()\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "for i, col in enumerate(numerical_cols):\n",
    "    sns.histplot(df[col], kde=True, ax=axes[i])\n",
    "    axes[i].set_title(f'Distribution of {col}')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645825d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# distributions for categorical features\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "for i, col in enumerate(categorical_cols):\n",
    "    sns.countplot(x=df[col], ax=axes[i])\n",
    "    axes[i].set_title(f'Distribution of {col}')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010839e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix for numerical features\n",
    "correlation_matrix = df[numerical_cols].corr()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()\n",
    "\n",
    "summary_stats, class_distribution\n",
    "#after encoding recreate- didnt add report bcs it doesnt contains target value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fc7f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#box plot for num values\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "for i, col in enumerate(numerical_cols):\n",
    "    sns.boxplot(x='class', y=col, data=df, ax=axes[i])\n",
    "    axes[i].set_title(f'Box Plot of {col} by Class')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b2a8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bar plot for cat columns\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "for i, col in enumerate(categorical_cols):\n",
    "    sns.countplot(x=col, hue='class', data=df, ax=axes[i])\n",
    "    axes[i].set_title(f'Bar Plot of {col} by Class')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6d43df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Data Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c97995",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = df.select_dtypes(include=['object']).columns\n",
    "categorical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40de6b0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "###to create balanced target variable values the 50-50 split\n",
    "poisonous = df[df['class'] == 'p']\n",
    "edible = df[df['class'] == 'e']\n",
    "\n",
    "#Determine the size of the smaller class\n",
    "min_size = min(len(poisonous), len(edible))\n",
    "\n",
    "# the larger class to match the size of the smaller class\n",
    "poisonous_downsampled = poisonous.sample(min_size, random_state=42)\n",
    "edible_downsampled = edible.sample(min_size, random_state=42)\n",
    "\n",
    "# Concatenate the balanced subsets\n",
    "balanced_df = pd.concat([poisonous_downsampled, edible_downsampled])\n",
    "\n",
    "\n",
    "balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(balanced_df['class'].value_counts())\n",
    "\n",
    "\n",
    "print(balanced_df.head())\n",
    "print(balanced_df.info())\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.countplot(data=balanced_df, x='class')\n",
    "plt.title('Distribution of Mushroom Classes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e94a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = df.drop(['class'], axis=1)\n",
    "y = df['class']\n",
    "\n",
    "# Convert categorical variables to dummy variables (one-hot encoding)\n",
    "X = pd.get_dummies(X)\n",
    "X.head()\n",
    "\n",
    "# Encode the target variable using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "boolean_columns = X.select_dtypes(include=['bool']).columns\n",
    "X[boolean_columns] = X[boolean_columns].astype(int)\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "#scaling\n",
    "scaler = MinMaxScaler()\n",
    "categorical_col = ['cap-diameter', 'stem-height', 'stem-width']\n",
    "\n",
    "\n",
    "X[categorical_col] = scaler.fit_transform(X[categorical_col])\n",
    "\n",
    "X[categorical_col].head()\n",
    "\n",
    "\n",
    "# Split the data as training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "print(X_train.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d619f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Classes###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45034585",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeNode:\n",
    "    def __init__(self, data=None, feature_idx=None, feature_val=None, prediction_probs=None, impurity=None, is_leaf=False):\n",
    "        self.data = data\n",
    "        self.feature_idx = feature_idx\n",
    "        self.feature_val = feature_val\n",
    "        self.prediction_probs = prediction_probs\n",
    "        self.impurity = impurity\n",
    "        self.is_leaf = is_leaf\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "\n",
    "    def decision_criterion(self, data_point):\n",
    "        if self.is_leaf:\n",
    "            return None\n",
    "        return data_point[self.feature_idx] < self.feature_val\n",
    "\n",
    "    def node_def(self):\n",
    "        return f'Feature Index: {self.feature_idx}, Feature Value: {self.feature_val}, Impurity: {self.impurity}, Prediction Probs: {self.prediction_probs}, Is Leaf: {self.is_leaf}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6888172",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DecisionTree(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, max_depth=4, min_samples_leaf=1, min_information_gain=0.0, criterion='entropy'):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.min_information_gain = min_information_gain\n",
    "        self.criterion = criterion\n",
    "        self.feature_importances_ = None\n",
    "\n",
    "    def entropy(self, class_probabilities):\n",
    "        return sum([-p * np.log2(p) for p in class_probabilities if p > 0])\n",
    "\n",
    "    def gini_index(self, class_probabilities):\n",
    "        return 1 - sum([p ** 2 for p in class_probabilities])\n",
    "\n",
    "    def variance_reduction(self, subsets):\n",
    "        total_count = sum([len(subset) for subset in subsets])\n",
    "        if total_count == 0:\n",
    "            return 0\n",
    "        total_variance = np.var(np.concatenate(subsets)) if total_count > 0 else 0\n",
    "        weighted_variances = sum([np.var(subset) * len(subset) / total_count for subset in subsets if len(subset) > 0])\n",
    "        return total_variance - weighted_variances\n",
    "\n",
    "    def class_probabilities(self, labels):\n",
    "        total_count = len(labels)\n",
    "        return [label_count / total_count for label_count in Counter(labels).values()]\n",
    "\n",
    "    def data_impurity(self, labels):\n",
    "        class_probs = self.class_probabilities(labels)\n",
    "        if self.criterion == 'entropy':\n",
    "            return self.entropy(class_probs)\n",
    "        elif self.criterion == 'gini':\n",
    "            return self.gini_index(class_probs)\n",
    "        elif self.criterion == 'variance_reduction':\n",
    "            return np.var(labels) if len(labels) > 0 else 0\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown criterion: {self.criterion}\")\n",
    "\n",
    "    def partition_impurity(self, subsets):\n",
    "        total_count = sum([len(subset) for subset in subsets])\n",
    "        if self.criterion == 'variance_reduction':\n",
    "            return self.variance_reduction(subsets)\n",
    "        return sum([self.data_impurity(subset) * (len(subset) / total_count) for subset in subsets])\n",
    "\n",
    "    def split(self, data, feature_idx, feature_val):\n",
    "        mask_below_threshold = data[:, feature_idx] < feature_val\n",
    "        group1 = data[mask_below_threshold]\n",
    "        group2 = data[~mask_below_threshold]\n",
    "        return group1, group2\n",
    "\n",
    "    def find_best_split(self, data):\n",
    "        min_part_impurity = float('inf')\n",
    "        min_impurity_feature_idx = None\n",
    "        min_impurity_feature_val = None\n",
    "\n",
    "        for idx in range(data.shape[1] - 1):\n",
    "            feature_val = np.median(data[:, idx])\n",
    "            g1, g2 = self.split(data, idx, feature_val)\n",
    "            part_impurity = self.partition_impurity([g1[:, -1], g2[:, -1]])\n",
    "            if part_impurity < min_part_impurity:\n",
    "                min_part_impurity = part_impurity\n",
    "                min_impurity_feature_idx = idx\n",
    "                min_impurity_feature_val = feature_val\n",
    "                g1_min, g2_min = g1, g2\n",
    "\n",
    "        return g1_min, g2_min, min_impurity_feature_idx, min_impurity_feature_val, min_part_impurity\n",
    "\n",
    "    def find_label_probs(self, data):\n",
    "        labels_as_integers = data[:, -1].astype(int)\n",
    "        total_labels = len(labels_as_integers)\n",
    "        label_probabilities = np.zeros(len(self.labels_in_train), dtype=float)\n",
    "\n",
    "        for i, label in enumerate(self.labels_in_train):\n",
    "            label_index = np.where(labels_as_integers == label)[0]\n",
    "            if len(label_index) > 0:\n",
    "                label_probabilities[i] = len(label_index) / total_labels\n",
    "\n",
    "        return label_probabilities\n",
    "\n",
    "    def create_tree(self, data, current_depth):\n",
    "        if current_depth >= self.max_depth or len(np.unique(data[:, -1])) == 1:\n",
    "            is_leaf = True\n",
    "            label_probabilities = self.find_label_probs(data)\n",
    "            node_impurity = self.data_impurity(data[:, -1])\n",
    "            return TreeNode(data, prediction_probs=label_probabilities, impurity=node_impurity, is_leaf=is_leaf)\n",
    "\n",
    "        split_1_data, split_2_data, split_feature_idx, split_feature_val, split_impurity = self.find_best_split(data)\n",
    "        label_probabilities = self.find_label_probs(data)\n",
    "        node_impurity = self.data_impurity(data[:, -1])\n",
    "        information_gain = node_impurity - split_impurity\n",
    "\n",
    "        if self.feature_importances_ is None:\n",
    "            self.feature_importances_ = np.zeros(data.shape[1] - 1)\n",
    "        self.feature_importances_[split_feature_idx] += information_gain\n",
    "        print(f\"Feature {split_feature_idx} importance updated by {information_gain} (total: {self.feature_importances_[split_feature_idx]})\")\n",
    "\n",
    "\n",
    "        node = TreeNode(data, split_feature_idx, split_feature_val, label_probabilities, split_impurity)\n",
    "\n",
    "        if self.min_samples_leaf > split_1_data.shape[0] or self.min_samples_leaf > split_2_data.shape[0]:\n",
    "            node.is_leaf = True\n",
    "            return node\n",
    "        elif information_gain < self.min_information_gain:\n",
    "            node.is_leaf = True\n",
    "            return node\n",
    "\n",
    "        current_depth += 1\n",
    "        node.left = self.create_tree(split_1_data, current_depth)\n",
    "        node.right = self.create_tree(split_2_data, current_depth)\n",
    "\n",
    "        return node\n",
    "\n",
    "    def predict_one_sample(self, X):\n",
    "        node = self.tree\n",
    "\n",
    "        while node and not node.is_leaf:\n",
    "            if node.decision_criterion(X):\n",
    "                node = node.left\n",
    "            else:\n",
    "                node = node.right\n",
    "\n",
    "        return node.prediction_probs if node else None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.labels_in_train = np.unique(y)\n",
    "        train_data = np.concatenate((X, np.reshape(y, (-1, 1))), axis=1)\n",
    "        self.tree = self.create_tree(data=train_data, current_depth=0)\n",
    "        self.feature_importances_ /= np.sum(self.feature_importances_)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        pred_probs = np.apply_along_axis(self.predict_one_sample, 1, X)\n",
    "        return pred_probs\n",
    "\n",
    "    def predict(self, X):\n",
    "        pred_probs = self.predict_proba(X)\n",
    "        preds = np.argmax(pred_probs, axis=1)\n",
    "        return preds\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {\"max_depth\": self.max_depth, \"min_samples_leaf\": self.min_samples_leaf, \"min_information_gain\": self.min_information_gain, \"criterion\": self.criterion}\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        for param, value in params.items():\n",
    "            setattr(self, param, value)\n",
    "        return self\n",
    "\n",
    "    def print_recursive(self, node, level=0):\n",
    "        if node is not None:\n",
    "            self.print_recursive(node.left, level + 1)\n",
    "            print('    ' * 4 * level + '-> ' + node.node_def())\n",
    "            self.print_recursive(node.right, level + 1)\n",
    "\n",
    "    def print_tree(self):\n",
    "        self.print_recursive(node=self.tree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc31302",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    predictions = model.predict(X_test)\n",
    "    accuracy = np.mean(predictions == y_test)\n",
    "    print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, predictions))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, predictions))\n",
    "\n",
    "dt_entropy = DecisionTree(max_depth=13, min_samples_leaf=4, min_information_gain=0.01, criterion='entropy')\n",
    "dt_entropy.fit(X_train, y_train)\n",
    "print(\"Entropy Criterion:\")\n",
    "evaluate_model(dt_entropy, X_test, y_test)\n",
    "\n",
    "dt_gini = DecisionTree(max_depth=13, min_samples_leaf=4, min_information_gain=0.01, criterion='gini')\n",
    "dt_gini.fit(X_train, y_train)\n",
    "print(\"\\nGini Criterion:\")\n",
    "evaluate_model(dt_gini, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb925322",
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree = DecisionTree()\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [7, 10, 13, 15],\n",
    "    'min_samples_leaf': [1,2,4,6],\n",
    "    'min_information_gain': [0, 0.01, 0.1],\n",
    "    'criterion': ['entropy', 'gini']\n",
    "}\n",
    "\n",
    "\n",
    "#  RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(decision_tree, param_distributions=param_grid, n_iter=50, cv=5, scoring='accuracy', n_jobs=-1, random_state=48)\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "#  the best parameters and best score\n",
    "best_params = random_search.best_params_\n",
    "best_score = random_search.best_score_\n",
    "\n",
    "# Use the best parameters to train the final model\n",
    "best_dtc = random_search.best_estimator_\n",
    "best_dtc.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "test_accuracy = best_dtc.score(X_test, y_test)\n",
    "train_accuracy = best_dtc.score(X_train, y_train)\n",
    "test_accuracy = best_dtc.score(X_test, y_test)\n",
    "\n",
    "\n",
    "cv_results = random_search.cv_results_\n",
    "\n",
    "train_sizes, train_scores, val_scores = learning_curve(\n",
    "    best_dtc, X_train, y_train, cv=5, scoring='accuracy', n_jobs=-1,\n",
    "    train_sizes=np.linspace(0.1, 1.0, 10)\n",
    ")\n",
    "\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "val_scores_mean = np.mean(val_scores, axis=1)\n",
    "val_scores_std = np.std(val_scores, axis=1)\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Learning Curves')\n",
    "plt.xlabel('Training examples')\n",
    "plt.ylabel('Score')\n",
    "plt.grid()\n",
    "\n",
    "plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                 train_scores_mean + train_scores_std, alpha=0.1, color='r')\n",
    "plt.fill_between(train_sizes, val_scores_mean - val_scores_std,\n",
    "                 val_scores_mean + val_scores_std, alpha=0.1, color='g')\n",
    "plt.plot(train_sizes, train_scores_mean, 'o-', color='r', label='Training score')\n",
    "plt.plot(train_sizes, val_scores_mean, 'o-', color='g', label='Cross-validation score')\n",
    "\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916b8522",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Best Parameters: {best_params}')\n",
    "print(f'Best Cross-validation Accuracy: {best_score * 100:.2f}%')\n",
    "print(f'Training Accuracy: {train_accuracy * 100:.2f}%')\n",
    "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n",
    "print(\"Cross-Validation Mean Scores: \", cv_results['mean_test_score'])\n",
    "print(\"Cross-Validation Std Dev: \", cv_results['std_test_score'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89c8c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [3,5,7,10,12,13],\n",
    "    'min_samples_leaf': [4,5,6,8,10],\n",
    "    'min_information_gain': [0.005,0.01,0.05, 0.1],\n",
    "    'criterion': ['entropy', 'gini']\n",
    "}\n",
    "\n",
    "\n",
    "random_search = RandomizedSearchCV(decision_tree, param_distributions=param_grid, n_iter=96, cv=5, scoring='accuracy', n_jobs=-1, random_state=48)\n",
    "\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "best_params = random_search.best_params_\n",
    "best_score = random_search.best_score_\n",
    "\n",
    "\n",
    "best_dtc = random_search.best_estimator_\n",
    "best_dtc.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "train_accuracy = best_dtc.score(X_train, y_train)\n",
    "test_accuracy = best_dtc.score(X_test, y_test)\n",
    "cv_results = random_search.cv_results_\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_sizes, train_scores, val_scores = learning_curve(\n",
    "    best_dtc, X_train, y_train, cv=5, scoring='accuracy', n_jobs=-1,\n",
    "    train_sizes=np.linspace(0.1, 1.0, 10)\n",
    ")\n",
    "\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "val_scores_mean = np.mean(val_scores, axis=1)\n",
    "val_scores_std = np.std(val_scores, axis=1)\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Learning Curves')\n",
    "plt.xlabel('Training examples')\n",
    "plt.ylabel('Score')\n",
    "plt.grid()\n",
    "\n",
    "plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                 train_scores_mean + train_scores_std, alpha=0.1, color='r')\n",
    "plt.fill_between(train_sizes, val_scores_mean - val_scores_std,\n",
    "                 val_scores_mean + val_scores_std, alpha=0.1, color='g')\n",
    "plt.plot(train_sizes, train_scores_mean, 'o-', color='r', label='Training score')\n",
    "plt.plot(train_sizes, val_scores_mean, 'o-', color='g', label='Cross-validation score')\n",
    "\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9481381",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f'Best Parameters: {best_params}')\n",
    "print(f'Best Cross-validation Accuracy: {best_score * 100:.2f}%')\n",
    "print(f'Training Accuracy: {train_accuracy * 100:.2f}%')\n",
    "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n",
    "print(\"Cross-Validation Mean Scores: \", cv_results['mean_test_score'])\n",
    "print(\"Cross-Validation Std Dev: \", cv_results['std_test_score'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2338230",
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree = DecisionTree()\n",
    "param_grid = {\n",
    "    'max_depth': [7, 10, 12, 15],\n",
    "    'min_samples_leaf': [2, 4, 6, 8],\n",
    "    'min_information_gain': [0.005, 0.01, 0.05],\n",
    "    'criterion': ['entropy', 'gini']\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(decision_tree, param_distributions=param_grid, n_iter=92, cv=5, scoring='accuracy', n_jobs=-1, random_state=48)\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "best_params = random_search.best_params_\n",
    "best_score = random_search.best_score_\n",
    "\n",
    "\n",
    "best_dtc = random_search.best_estimator_\n",
    "best_dtc.fit(X_train, y_train)\n",
    "\n",
    "test_accuracy = best_dtc.score(X_test, y_test)\n",
    "\n",
    "train_accuracy = best_dtc.score(X_train, y_train)\n",
    "test_accuracy = best_dtc.score(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "cv_results = random_search.cv_results_\n",
    "\n",
    "\n",
    "train_sizes, train_scores, val_scores = learning_curve(\n",
    "    best_dtc, X_train, y_train, cv=5, scoring='accuracy', n_jobs=-1,\n",
    "    train_sizes=np.linspace(0.1, 1.0, 10)\n",
    ")\n",
    "\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "val_scores_mean = np.mean(val_scores, axis=1)\n",
    "val_scores_std = np.std(val_scores, axis=1)\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Learning Curves')\n",
    "plt.xlabel('Training examples')\n",
    "plt.ylabel('Score')\n",
    "plt.grid()\n",
    "\n",
    "plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                 train_scores_mean + train_scores_std, alpha=0.1, color='r')\n",
    "plt.fill_between(train_sizes, val_scores_mean - val_scores_std,\n",
    "                 val_scores_mean + val_scores_std, alpha=0.1, color='g')\n",
    "plt.plot(train_sizes, train_scores_mean, 'o-', color='r', label='Training score')\n",
    "plt.plot(train_sizes, val_scores_mean, 'o-', color='g', label='Cross-validation score')\n",
    "\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2640be61",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Best Parameters: {best_params}')\n",
    "print(f'Best Cross-validation Accuracy: {best_score * 100:.2f}%')\n",
    "print(f'Training Accuracy: {train_accuracy * 100:.2f}%')\n",
    "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n",
    "print(\"Cross-Validation Mean Scores: \", cv_results['mean_test_score'])\n",
    "print(\"Cross-Validation Std Dev: \", cv_results['std_test_score'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c3d7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = best_dtc.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "feature_names = X_train.columns\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.title(\"Feature Importances\")\n",
    "plt.bar(range(X_train.shape[1]), importances[indices], align=\"center\")\n",
    "plt.xticks(range(X_train.shape[1]), feature_names[indices], rotation=90)\n",
    "plt.xlim([-1, X_train.shape[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed3c2b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
